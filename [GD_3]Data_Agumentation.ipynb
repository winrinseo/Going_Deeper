{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[GD-3]Data_Agumentation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 셋의 현실\n",
        "딥러닝 모델을 학습시키기 위해 대량의 데이터셋이 필요합니다.  \n",
        "앞서 사용했었던 이미지넷의 경우 1400만장의 이미지를 보유하고 있으며 cifal-10만 해도 5만장의 데이터를 가지고 있습니다.\n",
        "label 데이터를 포함해 장당 10원이라 해도 이미지넷의 경우 약 1억이상의 비용이 들어갑니다.\n",
        "\n",
        "또, 데이터를 만장 단위로 수집하는 것도 쉽지 않습니다.\n",
        "\n",
        "따라서 가지고 있는 데이터를 최대한 활용하는 방법인 Data augmentation을 알아봅니다.\n",
        "\n",
        "## Data augmentation\n",
        "Data augmentation은 가지고 있는 데이터를 증강시켜 학습 데이터 규모를 키울 수 있는 방법입니다.  \n",
        "데이터가 많아진다는 것은 overfitting을 줄일 수 있다는 것을 의미합니다.\n",
        "또, 데이터셋과 실제 상황에서의 입력값이 다를 경우 augmentation을 통해 실제 입력값과 비슷한 분포를 만들어 낼 수 있습니다.  \n",
        "예를 들어 학습 데이터는 노이즈가 없었지만 테스트 데이터에서는 노이즈가 있는 경우, augmentaion을 통해 노이즈를 삽입해 모델이 이런 노이즈에 잘 대응할 수 있도록 할 수 있습니다."
      ],
      "metadata": {
        "id": "5sjmOAo-If0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0GSBjL0xNA_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}